{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Investigating Heart disease**"},{"metadata":{},"cell_type":"markdown","source":"Heart disease is the leading cause of death across America. If we can understand it better using statistical models we can hopefully devise methods to detect the signs early. There are many different factors than can contribute to this disease, some of which may be missing from the dataset I am working with due mainly to the sheer number of them. However, using what we have can shed plenty of light on the nature of this problem. \n\nThere are two goals in this project.The first is to get a better understanding of how some of these factors contribute to the overall scheme of things. The second is to build and validate models that will allow us to classify whether or not a patient has heart disease based on their symptoms.\n\nLets get started by first setting up our required libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier as tree\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading and Cleaning the Data**"},{"metadata":{},"cell_type":"markdown","source":"Here is where we gather our data from our csv file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/shuffled-heart/shuffled_heart.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing to note from an initial inspection is that some of the data is represented as dummy variables. The descriptions for these variables can be found in the dataset's documentation. "},{"metadata":{},"cell_type":"markdown","source":"Before we proceed, it is a good idea to check whether or not we need to clean up the data. The following code checks if there is any missing entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully all the columns are filled in; however, I want to make the data more intuitive to look at, thus we can replace the dummy variables using the dataset's documentation. Despite my desire to make the data more intuitive, it would be wise to keep all our dummy variables intact to avoid a headache when we are ready to build our models. Therefore, I will make a copy of the dataframe and fill it in with intuitive information. The 'intuitive' dataframe will be used for data exploration purposes, the original dataframe will be used for statistical analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"intuitive = data.copy()\n\nintuitive['cp'] = intuitive['cp'].replace(0,'No pain')\nintuitive['cp'] = intuitive['cp'].replace(1,'Typical-agina')\nintuitive['cp'] = intuitive['cp'].replace(2,'Atypical-agina')\nintuitive['cp'] = intuitive['cp'].replace(3,'Non-agina')\nintuitive['cp'] = intuitive['cp'].replace(4,'Asymptomatic')\nintuitive['sex'] = intuitive['sex'].replace(1,'male')\nintuitive['sex'] = intuitive['sex'].replace(0,'female')\nintuitive['fbs'] = intuitive['fbs'].replace(0,'Under 120 mg/dl')\nintuitive['fbs'] = intuitive['fbs'].replace(1,'Over 120 mg/dl')\nintuitive ['restecg'] = intuitive['restecg'].replace(0,'Normal')\nintuitive ['restecg'] = intuitive['restecg'].replace(1,'Abnormal')\nintuitive['exang'] = intuitive['exang'].replace(1,'yes')\nintuitive['exang'] = intuitive['exang'].replace(0,'no')\nintuitive['target'] = intuitive['target'].replace(0,'No Heart Disease')\nintuitive['target'] = intuitive['target'].replace(1,'Has Heart Disease')\n\nintuitive.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this is better. We are ready to begin data exploration."},{"metadata":{},"cell_type":"markdown","source":"**Data exploration and visualization**"},{"metadata":{},"cell_type":"markdown","source":"First up, I'll load in the data and construct a heatmap. This heatmap can help us visualize what characteristics of heart disease are correlated."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data.corr(), annot = True, linewidths=.5, ax = ax).set_title(\"Correlation of Physical Characteristics\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suprisingly there are no strikingly high correlations between any two variables. The two highest correlation values for target come from thalach( maximum heartrate achieved) and cp( chest pain type). I expected the top two to come from blood pressure and cholesterol levels as these factors directly relate to the heart yet these have relatively low correlations to diagnosis.  Let us see some statistics for chest pain type and maximum heart rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cp'].mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['thalach'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the chest pain data was qualitative I was more interested in what was the most common chest pain type. It turns out typical agina was the most common type of chest pain. This type of pain can be identified by tightness in the chest area, shortness of breath and a sudden increase of perspiration. This chest pain is caused by blockages in the arteries resulting in the heart not getting enough oxygen. (Reference: https://www.harringtonhospital.org/typical-and-atypical-angina-what-to-look-for/)\n\nLooking at the data for maximum heart rate, the average was 150 bpm. Using the formula well known formula MAX_HEARTRATE = (220 - AGE) we find that this average equates to the maximum heart rate for a 72 year old person. Now this begs the question: what does the age distribution look like?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,4))\nax = sns.countplot(x = data['age']).set_title('Age distribuition of patients studied')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that is very interesting. Most people studied were in the age range of 50 through 60 and only a handful of patients were in their 70's. This can only mean that a majority of these patients exhibited a lower maximum heart rate than expected for their age. Why is this? Fortunately, that answer can be answered from our previous observation of chest pain. Remember, the most common type of chest pain is caused by semi clogged arteries. With less oxygen flowing into the heart, the heart will have a difficult time pumping out blood, hence a lower maximum heart rate. This also gives a physical explanation for why chest pain and heart rate had a noticible correlation on the heat map."},{"metadata":{},"cell_type":"markdown","source":"I now want to explore if a person's gender has any effect on anything. First lets look at the hand we're dealt."},{"metadata":{"trusted":true},"cell_type":"code","source":"intuitive['sex'].value_counts().plot.bar(title = 'Genders of patients studied')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intuitive['sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were over twice the amount of male patients than there we female. This could be a bit problematic. If males exhibit different symptoms than women this would make this data biased towards males and it wouldnt fully capture what is going behind the scenes for females. \n\nLet us investigate the data gender specifically. First I will split the data by gender.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"is_female = intuitive['sex'] == 'female'\nis_male = intuitive['sex'] == 'male'\n\nall_females = intuitive[is_female]\nall_males = intuitive[is_male]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us revisit the age distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,4))\nsns.distplot(all_females['age']).set_title('Age Distribution of Females')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (11,4))\nsns.distplot( all_males['age']).set_title('Age Distribution of Males')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots we can see that males tend to report symptoms of heart disease at an earlier age than women. The distritbuition peaks in the late 50's for males and in the early 60's for females.  "},{"metadata":{},"cell_type":"markdown","source":"Now let's observe if the chest pains differ between genders."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = all_females['cp'].value_counts().index,y = all_females['cp'].value_counts(),palette = \"rocket\").set_title('Chest Pain Types Reported By Females')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = all_males['cp'].value_counts().index,y = data['cp'].value_counts(),palette = \"deep\").set_title('Chest Pain Types Reported By Males')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now isn't that strange? No chest pain is the most common occurance. Yet, the females and males both exhibit the same distribution of pain types reported. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = \"whitegrid\")\nsns.violinplot(x = intuitive['sex'], y = intuitive['thalach']).set_title(\"Maximum Heart Rate\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The median values and probability densities for the median for both genders are nearly the same, which is to be expected. One thing to notice though is that males only have one peak at the median. Females, on the other hand, have a second peak near the 125 bpm range. \nThe last two pieces of analysis confirm there is a noticble difference between male and female symptoms. It would be wise to re-analyze the correlations but in a gender specific way."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data[is_female].corr(), annot = True, linewidths=.5, ax = ax).set_title(\"Correlation For Females\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data[is_male].corr(), annot = True, linewidths=.5, ax = ax).set_title(\"Correlation For Males\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though not strikingly different from the original correlation heat map, we do notice a few things. Chest pain has a higher correlation to the females' diagnosis than males. Also, maximum heart rate was not the second most significant correlation for the diagnosis of females, it was slope which measures peak performance of exercise. What does this mean? Typical-agina pain is often induced through strenuous activity, including excercise. Since our data shows that females are extremely likely to feel this kind of pain we can infer that older females who have a underlying heart disease exercise put themselves at risk of inducing chest pain.\n\nFrom our analysis we were able to make some very interesting observations. Now that we're familiar enough with the data, it's time to model it."},{"metadata":{},"cell_type":"markdown","source":"**Preparing for modeling**"},{"metadata":{},"cell_type":"markdown","source":"When I first downloaded the dataset it was too neatly sorted. Specifically, all the rows with the same diagnosis had been grouped together. Thus I shuffled the original dataset and saved it as a new .csv file. That shuffled file is the one we are working with now. "},{"metadata":{},"cell_type":"markdown","source":"**K-Fold Cross Validation** "},{"metadata":{},"cell_type":"markdown","source":"We are dealing with a binary classification problem that is not neccisarily well separated. If I had to guess, I'd say our best bet would be logistic regression but lets quantify that theory against other popular classification models. K-fold cross validation can help out with that. In this case we will be using 5-fold cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['age','sex','cp','trestbps','chol','fbs','restecg','exang','oldpeak','slope','ca','thal']\ngoal = ['target']\n\nlogic_score = cross_val_score(LogisticRegression(), data[cols], data[goal], cv = 5,scoring = 'accuracy')\nl1_logic = cross_val_score(LogisticRegression(penalty = 'l1'), data[cols], data[goal], cv = 5,scoring = 'accuracy')\nlda_score = cross_val_score(LinearDiscriminantAnalysis(solver = 'svd'),data[cols],data[goal],cv = 5, scoring = 'accuracy')\nsvc_lin_score = cross_val_score(SVC(kernel = 'linear', C = 1), data[cols], data[goal], cv = 5,scoring = 'accuracy')\ntree_score = cross_val_score(tree(random_state = 0), data[cols], data[goal], cv = 5,scoring = 'accuracy')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('L2 Logistic Regression Score', logic_score.mean())\nprint('L1 Logistic Regression Score:', l1_logic.mean())\nprint('Linear Discriminant Analysis Score:', lda_score.mean())\nprint('Linear Support Vector Machine Score', svc_lin_score.mean())\nprint('Descision Tree Score', tree_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regresion, LDA and SVM seem to be the best fit for our model. Let's tweak these models to see if we can get a better score."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (11,4))\nplt.plot(logic_score, label = 'L2 Logistic Regression')\nplt.plot(l1_logic,label = 'L1 Logistic Regression')\nplt.plot(lda_score, label = 'Linear Discriminant Analysis')\nplt.plot(svc_lin_score, label = 'Support Vector Classification')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our best accuracies came from linear discriminant analysis and support vector classification both having an accuracy of 82%. It's time to experiment and tweak our features to see if we can improve the model."},{"metadata":{},"cell_type":"markdown","source":"**Transforming using LDA**"},{"metadata":{},"cell_type":"markdown","source":"Linear Discriminant Analysis aims to maximize seperability in our data while reducing the dimensions. Perhaps transforming the data using LDA and then applying a model could increase the accuracy of predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = np.split(data,[data['age'].count() // 2],axis = 0)\nX_train = train[cols]\ny_train = train[goal]\nX_test = test[cols]\ny_test = test[goal]\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train,y_train)\ntransformed = lda.transform(data[cols])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_df = pd.concat([pd.DataFrame(transformed),data['target']] ,axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(transformed_df[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_logic_score = cross_val_score(LogisticRegression(), transformed, data[goal], cv = 5,scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_logic_score.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lda_logic_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_svc = cross_val_score(SVC(kernel = 'linear', C = 1), transformed, data[goal], cv = 5,scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lda_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_svc.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are definitely moving in the right direction! Logistic regression's accuracy increased by 4% and while support vector classification increased by 2%. "},{"metadata":{},"cell_type":"markdown","source":"**RFE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.feature_selection import RFE\nlogic_scores = []\nfor x in range(1,12):\n    logistic_rfe = RFE(LogisticRegression(), x, step=1)\n    logistic_rfe.fit(X_train,y_train)\n    logic_scores.append(logistic_rfe.score(X_test,y_test))\nlogic_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(logic_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scaling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit_transform(data)\ndf_scaled = pd.DataFrame(data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strain,stest = np.split(df_scaled,[df_scaled[1].count() // 2],axis = 0)\ns_cols = [0,1,2,3,4,5,6,7,8,9,10,11,12]\ns_goal = [13]\nXScale_train = strain[s_cols]\nyScale_train = strain[s_goal]\nXScale_test = stest[s_cols]\nyScale_test = stest[s_goal]\nscaled_logic_score = cross_val_score(LogisticRegression(), df_scaled[s_cols], df_scaled[s_goal], cv = 5,scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_logic_score.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(scaled_logic_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Separation by gender**"},{"metadata":{},"cell_type":"markdown","source":"From our data exploration we know that the males out numbered the females. Perhaps separating the data by gender and making seperate models for each gender can improve our accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"fem = data[is_female]\nmales = data[is_male]\n\nfemale_logic_score = cross_val_score(LogisticRegression(), fem[cols], fem[goal], cv = 5,scoring = 'accuracy')\nmale_logic_score = cross_val_score(LogisticRegression(), males[cols], males[goal], cv = 5,scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(female_logic_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(male_logic_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Female Score:\", female_logic_score.mean())\nprint(\"Male Score:\", male_logic_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"We see a significant increase in the female model but the male model took a significant hit in accuracy. "},{"metadata":{},"cell_type":"markdown","source":"**PCA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprinciple_components = pca.fit_transform(data[cols])\npca_cols = ['principle component 1', 'principle component 2']\npca_df = pd.DataFrame(data = principle_components, columns = pca_cols)\npca_df = pd.concat([pca_df, data['target']], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_logic = cross_val_score(LogisticRegression(), pca_df[pca_cols], data[goal], cv = 5,scoring = 'accuracy')\npca_svc = cross_val_score(SVC(kernel = 'linear', C = 1), pca_df[pca_cols], pca_df['target'], cv = 5,scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('PCA Logistic:', pca_logic.mean())\nprint('PCA SVC:', pca_svc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ensemble Methods**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ntransformed_df = pd.DataFrame(transformed)\ntransformed_df = pd.concat([transformed_df,data['target']], axis = 1)\ntr_train,tr_test = np.split(transformed_df,[transformed_df[0].count() // 2],axis = 0)\nXT_train = tr_train[0]\nyt_train = tr_train['target']\nXT_test = tr_test[0]\nyt_test = tr_test['target']\nrfc = RandomForestClassifier()\nrfc.fit(XT_train.values.reshape(-1,1),yt_train.values.reshape(-1,1))\nrfc.score(XT_test.values.reshape(-1,1),yt_test.values.reshape(-1,1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X_train,y_train)\nrfc.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The xgboost library can help us develop a more accurate model using gradient boosting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(n_estimators = 100)\nxgb.fit(X_train ,y= y_train)\nxgb_predict = xgb.predict(X_test)\nxgb_mse = mean_squared_error(xgb_predict,y_test)\n\nrounded = []\nfor x in range(len(xgb_predict)):\n    rounded.append(int(xgb_predict[x].round()))\nr = np.asarray(rounded)\nte = y_test.as_matrix().tolist()\nacc = [0,0]\nfor x in range(len(r)):\n    acc[1]+= 1\n    if(te[x][0] == rounded[x]):\n        acc[0] += 1\naccuracy = acc[0] / acc[1] * 100\nprint('Gradient boosting MSE:',xgb_mse)\nprint('Accuracy:',accuracy )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = np.split(transformed_df,[data['age'].count() // 2],axis = 0)\nXT_train = train[0]\nyt_train = train['target']\nXT_test = test[0]\nyt_test = test['target']\n\ntxgb = XGBRegressor(n_estimators = 100)\ntxgb.fit(XT_train.values.reshape(-1,1),yt_train.values.reshape(-1,1))\ntxgb_predict = txgb.predict(XT_test.values.reshape(-1,1))\n#mean_squared_error(txgb_predict,yt_test)\nrounded = []\nfor x in range(len(txgb_predict)):\n    rounded.append(int(txgb_predict[x].round()))\nr = np.asarray(rounded)\nacc = [0,0]\nyte = yt_test.values.reshape(-1,1)\nfor x in range(len(r)):\n    acc[1]+= 1\n    if(yte[x][0] == rounded[x]):\n        acc[0] += 1\naccuracy = acc[0] / acc[1] * 100\naccuracy\n#yt_test.values.reshape(-1,1)[0][0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}